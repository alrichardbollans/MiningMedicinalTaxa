,precision,class,Model,NS
54,0.8367217280813215,Precise NER,Claude,False
55,0.8939008894536213,Approx. NER,Claude,False
72,0.8375796178343949,Precise NER,Claude,True
73,0.8949044585987261,Approx. NER,Claude,True
6,0.8973031283710895,Precise NER,Gemini,False
7,0.9216828478964402,Approx. NER,Gemini,False
24,0.9204722655379818,Precise NER,Gemini,True
25,0.9456449097794608,Approx. NER,Gemini,True
60,0.4177057356608479,Precise NER,Gnfinder,False
61,0.7066708229426434,Approx. NER,Gnfinder,False
66,0.5195868251802768,Precise NER,Gnfinder,True
67,0.8793607483921263,Approx. NER,Gnfinder,True
18,0.909170305676856,Precise NER,GPT,False
19,0.9301310043668122,Approx. NER,GPT,False
30,0.924545130217624,Precise NER,GPT,True
31,0.9459507670353192,Approx. NER,GPT,True
36,0.8889584637862659,Precise NER,Llama,False
37,0.918388645376748,Approx. NER,Llama,False
42,0.8976960473472839,Precise NER,Llama,True
43,0.9274994715704924,Approx. NER,Llama,True
0,0.2229107086113759,Precise NER,TaxoNERD,False
1,0.2657557462133249,Approx. NER,TaxoNERD,False
78,0.5397999725989862,Precise NER,TaxoNERD,True
79,0.6439238251815317,Approx. NER,TaxoNERD,True

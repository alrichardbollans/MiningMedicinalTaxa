---
title: "Named Entity Recognition with GPT-3 and GNFinder"
output:
  html_document: default
  word_document: default
  date: "2023-06-15"
---

# Data prep

## Load the necessary libraries

```{r}
library(openai) 
library(tabulizer)
library(dplyr) 
library(pdftools) 
library(writexl)
library(purrr) 
library(usethis) 
library(reticulate) 
library(tictoc)
library(tidyr)
library(stringr)

```

## Import document

```{r}

# Define the path
path<-"TraditionalMedicinAfrica.pdf"

# Get nunber of pages of PDF
p <- get_n_pages(path)

# Extract text from pdf
doc_input<- extract_text(path, pages = seq(1,p), encoding="UTF-8")

```

## 'lines' is a list of lines from a document that you want to process

```{r}

# Initialize lines as an empty list
lines<-list()

for(i in 1:length(doc_input)){
lines[[i]]<- unlist(strsplit(doc_input[[i]], "\r\n")) }

# Print doc input transformed in lines
lines[[i]] %>% head()

```

# GPT3

## Set your OpenAI API key

```{r}

# (Please replace 'my_api' with your actual OpenAI API key)


Sys.setenv(OPENAI_API_KEY = 'my_api') 
```

## Create GPT-3 prompt

```{r}

# Create a prompt to provide the GPT-3 model with the pattern of information input and desired output. 


pr<-"I am an excellent botanist. The task is to annotate plant names along with author. 
Below some examples:
Input:Abassian boxwood Buxus sempervirens
###
Output:Abassian boxwood @@Buxus sempervirens##
###
Input: black dye
###
Output: black dye
###
Input:Abaxianthus M.A. Clem. & D.L. Jones = Dendrobium
###
Output:@@Abaxianthus M.A. Clem. & D.L. Jones## = @@Dendrobium##
###
Input:"
```

## Use the prompt to prepare the list of inputs for the GPT-3 model

```{r}
# Initialize prompt as an empty list
prompt<- list() 

for (i in 1:length(lines)) { 
  prompt[[i]] <- list()
    for (j in 1:length(lines[[i]])) { 
      prompt[[i]][j] <- unlist(paste0(pr,lines[[i]][j], "\n###\nOutput:")) }
  }
  
```

```{r}
# Use 'tiktoken' to count how many tokens are in each prompt. 

# if required install tiktoken with py_install("tiktoken") 

tiktoken <-import("tiktoken") 
encoding <- tiktoken$encoding_for_model("gpt-3.5-turbo")

# Count tokens
# Initialize token as an empty list
token <- list()
for (i in 1:length(prompt)) {
    token[[i]] <- list()
    for (j in 1:length(prompt[[i]])) {
        token[[i]][j] <- 
          unlist(length(encoding$encode(prompt[[i]][[j]])))}
}

# Calculate total tokens and estimate the cost 
 token_length<-list()
 for (i in 1:length(lines))
    token_length[[i]]<-
      do.call(rbind, token[[i]]) %>% 
      as.data.frame() %>% 
      tibble::rowid_to_column() 

  total_tokens <- token_length %>% 
    do.call(rbind,.) %>%
    pull(V1)%>%
    sum() 
  # (1000 tokens cost roughly 0.02$) 
  estimated_cost <- total_tokens*2e-05 
```

```{r}
  
  paste0("tot tokens:",total_tokens)
  paste0("estimated cost:",round(estimated_cost,2),"$")
  
```

## Fit GPT-3 model

```{r}

tic() # start timing

# Initialize gpt as an empty list
gpt<- list()
model <-"text-davinci-003" 
 
  for (i in 1:length(lines)) {
   gpt[[i]] <- list()  # Create sub-lists for each element of gpt
   for (j in 1:length(lines[[i]])) {
     if (token_length[[i]]$V1[j] > 200) {
       next
     }
     Sys.sleep(1)  # Add a 1-second delay before each iteration
     
     # gpt[[i]][[j]] <- create_completion(model = model,
     #                                    prompt = prompt[[i]][[j]],
     #                                    top_p = 0.1,
     #                                    max_tokens = 200
                                        #stop = "###\n"
     #)
   }
 }
 
toc() # Stop timing

#4059.62 sec elapsed
```

## Extract gpt-3 completion results

```{r}

# Initialize page as an empty list
page<- list()

for (i in 1:length(gpt)){
  page[[i]] <- unlist(lapply(gpt[[i]], function(x) x$choices$text)) }

```

## Compare the original document with gpt-3 results

```{r}


list1<-page
list2<-lines


# Create data frames from the lists
df1 <-  map2_dfr(list1, seq_along(list1),~data.frame(Content = .x, Page = .y))
df2 <-  map2_dfr(list2, seq_along(list2),~data.frame(Content = .x, Page = .y))

# Name the columns
names(df1) <- c("gpt3", "page")
names(df2) <- c("original", "page")

# Combine data frames
df <- cbind(df2[1], df1) %>% 
  mutate(gpt3=trimws(gpt3,"left")) %>%
  mutate(different=ifelse(original==gpt3,T,F))

df
```

## Save GPT-3 results in an Excel file

```{r}

write.table(df2 %>% select(original), "TMA.txt")
writexl::write_xlsx(df, "output.xlsx")

```

## Extracted names to be verified

```{r}
gpt3_extracted <- df1 %>%
  select(gpt3)%>%
  separate_rows(gpt3, sep = ",")%>%
  mutate(extracted= str_extract_all(gpt3, "@@(.*?)##"))%>%
  select(extracted)%>%
  unlist()%>%
  as_tibble()%>%
  mutate(value= str_replace_all(value, "@@|##", ""))
```

#GNFinder

## Extract plant names with gnfinder

```{r}

# The pdf version of TMA was transformed in text in this case

# Set URL and file paths
url <- "https://finder.globalnames.org/api/v1/find"
file_path <- "TMA.txt"  # Replace with your file path

# Create a list for multipart content
multi_part_content <- list(
  "sources[]" = "197",
  #"sources[]" = "12",
  "file" = upload_file(file_path)
)

```

```{r}
# POST request
response <- POST(url, body = multi_part_content, verbose())

```

## Save GNFinder results in an Excel file

```{r}
# export GNFinder results
gnf_extracted<-list()
for(i in 1:length(content(response)[2]$names))
gnf_extracted[[i]]<-content(response)[2]$names[[i]]$name

gnf_extracted<-gnf_extracted %>% unlist() %>% as_tibble

write.csv(gnf_extracted, "gnf_extracted.csv")
```
